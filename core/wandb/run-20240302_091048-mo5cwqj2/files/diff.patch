diff --git a/.vscode/launch.json b/.vscode/launch.json
index f8b73e9..5bb6183 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -3,7 +3,7 @@
 	// Hover to view descriptions of existing attributes.
 	// For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
 	"version": "0.2.0",
-	"configurations": [
+	"argsurations": [
 		{
 			"name": "Python: Current File",
 			"type": "python",
diff --git a/config_embed.py b/config_embed.py
index 13a4685..b90cc59 100644
--- a/config_embed.py
+++ b/config_embed.py
@@ -1,6 +1,6 @@
 import os
 import argparse
-from yacs.config import CfgNode as CN
+from yacs.args import CfgNode as CN
 
 
 def set_cfg(cfg):
@@ -34,18 +34,18 @@ def set_cfg(cfg):
     return cfg
 
 
-# Principle means that if an option is defined in a YACS config object,
-# then your program should set that configuration option using cfg.merge_from_list(opts) and not by defining,
+# Principle means that if an option is defined in a YACS args object,
+# then your program should set that argsuration option using cfg.merge_from_list(opts) and not by defining,
 # for example, --train-scales as a command line argument that is then used to set cfg.TRAIN.SCALES.
 
 
 def update_cfg(cfg, args_str=None):
     parser = argparse.ArgumentParser()
-    parser.add_argument('--config', default="",
-                        metavar="FILE", help="Path to config file")
+    parser.add_argument('--args', default="",
+                        metavar="FILE", help="Path to args file")
     # opts arg needs to match set_cfg
     parser.add_argument("opts", default=[], nargs=argparse.REMAINDER,
-                        help="Modify config options using the command-line")
+                        help="Modify args options using the command-line")
 
     if isinstance(args_str, str):
         # parse from a string
@@ -56,9 +56,9 @@ def update_cfg(cfg, args_str=None):
     # Clone the original cfg
     cfg = cfg.clone()
 
-    # Update from config file
-    if os.path.isfile(args.config):
-        cfg.merge_from_file(args.config)
+    # Update from args file
+    if os.path.isfile(args.args):
+        cfg.merge_from_file(args.args)
 
     # Update from command line
     cfg.merge_from_list(args.opts)
diff --git a/core/Embedding/ge/models/line.py b/core/Embedding/ge/models/line.py
index 04c5073..66a4025 100644
--- a/core/Embedding/ge/models/line.py
+++ b/core/Embedding/ge/models/line.py
@@ -96,7 +96,7 @@ class LINE:
         self._gen_sampling_table()
         self.reset_model()
 
-    def reset_training_config(self, batch_size, times):
+    def reset_training_args(self, batch_size, times):
         self.batch_size = batch_size
         self.steps_per_epoch = (
             (self.samples_per_epoch - 1) // self.batch_size + 1)*times
@@ -206,7 +206,7 @@ class LINE:
         return self._embeddings
 
     def train(self, batch_size=1024, epochs=1, initial_epoch=0, verbose=1, times=1):
-        self.reset_training_config(batch_size, times)
+        self.reset_training_args(batch_size, times)
         hist = self.model.fit_generator(self.batch_it, epochs=epochs, initial_epoch=initial_epoch, steps_per_epoch=self.steps_per_epoch,
                                         verbose=verbose)
 
diff --git a/core/Embedding/node2vec_tag.py b/core/Embedding/node2vec_tag.py
index 4cb0948..4ffde0b 100644
--- a/core/Embedding/node2vec_tag.py
+++ b/core/Embedding/node2vec_tag.py
@@ -19,11 +19,11 @@ from utils import get_git_repo_root_path, append_acc_to_excel, append_mrr_to_exc
 from ogb.linkproppred import PygLinkPropPredDataset, Evaluator
 from heuristic.eval import evaluate_auc, evaluate_hits, evaluate_mrr, get_metric_score, get_prediction
 from ge import Node2Vec
-from yacs.config import CfgNode as CN
+from yacs.args import CfgNode as CN
 import networkx as nx 
 import yaml
 from torch_geometric.graphgym.cmd_args import parse_args
-from torch_geometric.graphgym.config import (cfg)
+from torch_geometric.graphgym.args import (cfg)
 import graphgps 
 from heuristic.pubmed_heuristic import get_pubmed_casestudy
 from heuristic.cora_heuristic import get_cora_casestudy
@@ -120,10 +120,10 @@ def eval_embed(embed,  splits, visual=True):
     return y_pred, results_acc, results_mrr, y_test 
 
 
-def eval_pubmed_mrr_acc(config) -> None:
+def eval_pubmed_mrr_acc(args) -> None:
     """load text attribute graph in link predicton setting
     """
-    dataset, data_cited, splits = data_loader[config.data.name](config)
+    dataset, data_cited, splits = data_loader[args.data.name](args)
     
     # ust test edge_index as full_A
     full_edge_index = splits['test'].edge_index
@@ -140,15 +140,15 @@ def eval_pubmed_mrr_acc(config) -> None:
     
     result_dict = {}
     # Access individual parameters
-    walk_length = config.model.node2vec.walk_length
-    num_walks = config.model.node2vec.num_walks
-    p = config.model.node2vec.p
-    q = config.model.node2vec.q
-    workers = config.model.node2vec.workers
-    use_rejection_sampling = config.model.node2vec.use_rejection_sampling
-    embed_size = config.model.node2vec.embed_size
-    ws = config.model.node2vec.window_size
-    iter = config.model.node2vec.iter
+    walk_length = args.model.node2vec.walk_length
+    num_walks = args.model.node2vec.num_walks
+    p = args.model.node2vec.p
+    q = args.model.node2vec.q
+    workers = args.model.node2vec.workers
+    use_rejection_sampling = args.model.node2vec.use_rejection_sampling
+    embed_size = args.model.node2vec.embed_size
+    ws = args.model.node2vec.window_size
+    iter = args.model.node2vec.iter
 
     # model 
     for use_heuristic in ['node2vec']:
@@ -178,7 +178,7 @@ from heuristic.pubmed_heuristic import get_pubmed_casestudy
 # main function 
 if __name__ == "__main__":
     args = parse_args()
-    # Load config file
+    # Load args file
     
     cfg = set_cfg(args)
     cfg.merge_from_list(args.opts)
diff --git a/core/Embedding/node2vec_tagplus.py b/core/Embedding/node2vec_tagplus.py
index 02d6030..c0e5f43 100644
--- a/core/Embedding/node2vec_tagplus.py
+++ b/core/Embedding/node2vec_tagplus.py
@@ -54,6 +54,10 @@ from sklearn.metrics import accuracy_score
 from sklearn.manifold import TSNE
 import random 
 from numba.typed import List
+from torchmetrics.functional.retrieval import retrieval_hit_rate
+from torchmetrics.retrieval import RetrievalHitRate
+from torch_geometric.utils import to_scipy_sparse_matrix
+from IPython import embed
 
 FILE_PATH = get_git_repo_root_path() + '/'
 
@@ -111,52 +115,6 @@ def sample_n2v_random_walks(adj, walk_length, walks_per_node, p, q):
 from numba.typed import List 
 from numba import njit 
 
-# def _n2v_random_walk(indptr,
-#                     indices,
-#                     walk_length,
-#                     walks_per_node,
-#                     p,
-#                     q):
-#     N = len(indptr) - 1 # num of nodes
-#     final_walks = List() # all walks of one node 
-#     for _ in range(walks_per_node):
-#         for n_iter in range(N):
-#             walk = np.zeros(walk_length+1, dtype=np.int32)
-#             walk[0] = n_iter          
-
-#             visited_set = None
-#             for il in range(walk_length):
-#                 # v_iter 's neighbors
-#                 neighbors = indices[indptr[n_iter]:indptr[n_iter+1]]
-                
-#                 sample_idx_arr = np.zeros(len(neighbors)+1, dtype=np.int32)
-#                 sample_prob_arr = np.zeros(len(neighbors)+1, dtype=np.float32)
-                
-#                 for i, samples in enumerate(neighbors):
-#                     sample_idx_arr[i] = samples
-                
-#                     if visited_set is not None:
-#                         if samples in visited_set:
-#                             sample_prob_arr[i] = 1
-#                         else:
-#                             sample_prob_arr[i] = 1/q
-#                     else:
-#                         sample_prob_arr[i] = 1/q 
-
-#                 visited_set = neighbors.copy()
-#                 sample_idx_arr[-1] = n_iter
-#                 sample_prob_arr[-1] = 1/p
-                    
-#                 sample_prob_arr = sample_prob_arr / np.sum(sample_prob_arr)
-#                 n_iter = random_choice(sample_idx_arr, sample_prob_arr)
-                
-#                 walk[il+1] = n_iter
-
-#             final_walks.append(walk)
-#     return np.array(final_walks)
-
-
-
 
 
 # ### 用numba加速的版本
@@ -222,141 +180,21 @@ def random_choice(arr: np.int64, p):
 
 
 
-def evaluate_node_classification(embedding_matrix, labels, train_mask, 
-                                 test_mask, normalize_embedding=True, max_iter=1000):
-        
-    """训练一个线性模型（比如逻辑回归模型）来预测节点的标签
-    
-    返回值说明
-    ----
-    preds: 模型预测的标签
-    test_acc: 模型预测的准确率
-    """
-    ######################################
-    if normalize_embedding:
-        embedding_matrix = normalize(embedding_matrix)
-        
-    # split embedding
-    feature_train = embedding_matrix[train_mask]
-    feature_test = embedding_matrix[test_mask]
-    labels_train = labels[train_mask]
-    labels_test = labels[test_mask]
-    
-    clf = LogisticRegression(solver='lbfgs',max_iter=max_iter, multi_class='auto')
-    clf.fit(feature_train, labels_train)
-    
-    preds = clf.predict(feature_test)
-    test_acc = accuracy_score(labels_test, preds)
-    ######################################   
-    return preds, test_acc
-
-# 请大家完成下面这个测试函数
-def evaluate_link_prediction(embed, splits, normalize_embedding=True, max_iter=1000):
-        
-    """训练一个线性模型（比如逻辑回归模型）来预测节点的标签
-    
-    返回值说明
-    ----
-    preds: 模型预测的标签
-    test_acc: 模型预测的准确率
-    """
-    ######################################
-    # train loop
-
-    X_train_index, y_train = splits['train'].edge_label_index.T, splits['train'].edge_label
-    # dot product
-    X_train = embed[X_train_index]
-    X_train = np.multiply(X_train[:, 1], (X_train[:, 0]))
-    X_test_index, y_test = splits['test'].edge_label_index.T, splits['test'].edge_label
-    # dot product 
-    X_test = embed[X_test_index]
-    X_test = np.multiply(X_test[:, 1], (X_test[:, 0]))
-    
-    
-    
-    clf = LogisticRegression(solver='lbfgs',max_iter=max_iter, multi_class='auto')
-    clf.fit(X_train, y_train)
-    
-    y_pred = clf.predict(X_test)
-
-    acc = clf.score(X_test, y_test)
-
-    
-    results_acc = {'node2vec_acc': acc}
-    pos_test_pred = torch.tensor(y_pred[y_test == 1])
-    neg_test_pred = torch.tensor(y_pred[y_test == 0])
-    evaluator_hit = Evaluator(name='ogbl-collab')
-    evaluator_mrr = Evaluator(name='ogbl-citation2')
-    result_mrr = get_metric_score(evaluator_hit, evaluator_mrr, pos_test_pred, neg_test_pred)
-    results_mrr = {'node2vec_mrr': result_mrr}
-    
-    ######################################   
-    return y_pred, results_acc, results_mrr, y_test 
-
-def eval_pubmed_mrr_acc(config) -> None:
-    """load text attribute graph in link predicton setting
-    """
-    dataset, data_cited, splits = data_loader[config.data.name](config)
-    
-    # ust test edge_index as full_A
-    full_edge_index = splits['test'].edge_index
-    full_edge_weight = torch.ones(full_edge_index.size(1))
-    num_nodes = dataset._data.num_nodes
-    
-    m = construct_sparse_adj(full_edge_index)
-    plot_coo_matrix(m, f'test_edge_index.png')
-    
-    full_A = ssp.csr_matrix((full_edge_weight.view(-1), (full_edge_index[0], full_edge_index[1])), shape=(num_nodes, num_nodes)) 
-
-    evaluator_hit = Evaluator(name='ogbl-collab')
-    evaluator_mrr = Evaluator(name='ogbl-citation2')
-    
-    result_dict = {}
-    # Access individual parameters
-    walk_length = config.model.node2vec.walk_length
-    num_walks = config.model.node2vec.num_walks
-    p = config.model.node2vec.p
-    q = config.model.node2vec.q
-    workers = config.model.node2vec.workers
-    use_rejection_sampling = config.model.node2vec.use_rejection_sampling
-    embed_size = config.model.node2vec.embed_size
-    ws = config.model.node2vec.window_size
-    iter = config.model.node2vec.iter
-
-    # model 
-    for use_heuristic in ['node2vec']:
-        # G = nx.from_scipy_sparse_matrix(full_A, create_using=nx.Graph())
-        adj = to_scipy_sparse_matrix(full_edge_index)
-
-        embedding = node2vec(adj, embedding_dim=64, p=0.5, q=0.5)
-    
-
-
-    return evaluate_link_prediction(embedding, splits)
-
 
 # main function 
 if __name__ == "__main__":
-    from torch_geometric.datasets import Planetoid
-    from torch_geometric.utils import to_scipy_sparse_matrix
-    dataset = Planetoid(root='./data', name='Cora')# 将数据保存在data文件夹下
-    data = dataset[0]
-    adj = to_scipy_sparse_matrix(data.edge_index)
-
-    embedding = node2vec(adj, embedding_dim=64, p=0.5, q=0.5)
-    embedding.shape
 
     args = parse_args()
-    # Load config file
-    
-    config = set_cfg(args)
-    config.merge_from_list(args.opts)
-
+    # # Load args file
 
+    cfg = set_cfg(args)
+    embed()
+    cfg.merge_from_list(args.opts)
+    
     # Set Pytorch environment
-    torch.set_num_threads(config.num_threads)
+    torch.set_num_threads(cfg.num_threads)
     
-    dataset, data_cited, splits = data_loader[config.data.name](config)
+    dataset, data_cited, splits = data_loader[cfg.data.name](cfg)
     
     # ust test edge_index as full_A
     full_edge_index = splits['test'].edge_index
@@ -373,22 +211,23 @@ if __name__ == "__main__":
     
     result_dict = {}
     # Access individual parameters
-    walk_length = config.model.node2vec.walk_length
-    num_walks = config.model.node2vec.num_walks
-    p = config.model.node2vec.p
-    q = config.model.node2vec.q
-    workers = config.model.node2vec.workers
-    use_rejection_sampling = config.model.node2vec.use_rejection_sampling
-    embed_size = config.model.node2vec.embed_size
-    ws = config.model.node2vec.window_size
-    iter = config.model.node2vec.iter
+    walk_length = cfg.model.node2vec.walk_length
+    num_walks = cfg.model.node2vec.num_walks
+    p = cfg.model.node2vec.p
+    q = cfg.model.node2vec.q
+    workers = cfg.model.node2vec.workers
+    use_rejection_sampling = cfg.model.node2vec.use_rejection_sampling
+    embed_size = cfg.model.node2vec.embed_size
+    ws = cfg.model.node2vec.window_size
+    iter = cfg.model.node2vec.iter
     
     # G = nx.from_scipy_sparse_matrix(full_A, create_using=nx.Graph())
     adj = to_scipy_sparse_matrix(full_edge_index)
 
-    embed = node2vec(adj, embedding_dim=64, p=0.5, q=0.5)
+    embed = node2vec(adj, embedding_dim=embed_size, p=p, q=q)
         
-    
+    # TODO different methods to generate node embeddings
+    # embedding method 
     X_train_index, y_train = splits['train'].edge_label_index.T, splits['train'].edge_label
     # dot product
     X_train = embed[X_train_index]
@@ -399,21 +238,49 @@ if __name__ == "__main__":
     X_test = np.multiply(X_test[:, 1], (X_test[:, 0]))
     
     
-    
-    clf = LogisticRegression(solver='lbfgs',max_iter=iter, multi_class='auto')
+    clf = LogisticRegression(solver='lbfgs', max_iter=iter, multi_class='auto')
     clf.fit(X_train, y_train)
     
-    y_pred = clf.predict(X_test)
+    y_pred = clf.predict_proba(X_test)
 
     acc = clf.score(X_test, y_test)
 
+    plt.figure()
+    plt.plot(y_pred, label='pred')
+    plt.plot(y_test, label='test')
+    plt.savefig('node2vec_pred.png')
     
     results_acc = {'node2vec_acc': acc}
     pos_test_pred = torch.tensor(y_pred[y_test == 1])
     neg_test_pred = torch.tensor(y_pred[y_test == 0])
+    
     evaluator_hit = Evaluator(name='ogbl-collab')
     evaluator_mrr = Evaluator(name='ogbl-citation2')
-    result_mrr = get_metric_score(evaluator_hit, evaluator_mrr, pos_test_pred, neg_test_pred)
+    pos_pred = pos_test_pred[:, 1]
+    neg_pred = neg_test_pred[:, 1]
+    result_mrr = get_metric_score(evaluator_hit, evaluator_mrr, pos_pred, neg_pred)
     results_mrr = {'node2vec_mrr': result_mrr}
-
     print(results_acc, results_mrr)
+    
+
+    root = FILE_PATH + 'results'
+    acc_file = root + f'/{cfg.data.name}_acc.csv'
+    mrr_file = root +  f'/{cfg.data.name}_mrr.csv'
+    if not os.path.exists(root):
+        os.makedirs(root, exist_ok=True)
+    append_acc_to_excel(results_acc, acc_file, cfg.data.name)
+    append_mrr_to_excel(results_mrr, mrr_file)
+    
+    # TEST CODE
+    # indexes = torch.tensor(y_test.numpy().astype(int))
+    # preds = torch.tensor(y_pred[np.arange(len(indexes)), indexes])
+    # target = y_test != 0
+    
+    # hr2 = RetrievalHitRate(top_k=10)
+    # hits = hr2(preds, target, indexes=indexes)
+
+    # y_test = (y_test == 1)
+    # y_pred = torch.tensor(y_pred[:, 1])
+    # hits = [retrieval_hit_rate(y_pred, y_test, top_k=k) for k in [1, 3, 10, 100]]
+
+    
diff --git a/core/Embedding/wb_tune.py b/core/Embedding/wb_tune.py
index 8ce41f4..c839bfe 100644
--- a/core/Embedding/wb_tune.py
+++ b/core/Embedding/wb_tune.py
@@ -1,103 +1,130 @@
-
-from datetime import datetime as dt
-import argparse
-import uuid
-import torch
+# Import the W&B Python Library and log into W&B
 import wandb
 
-TF_ENABLE_ONEDNN_OPTS=0
-
-##################################################################################################################
-
-# Necessary changes before run this .py to tune hyper-parameters:
-
-# 1. change args.data with desired dataset
-#     e.g. datasets/RandomNoise_CDonly_new/training_data_5km.hdf5
-# 2. change args.csv_file_name
-#     e.g. tune on dataset Realistic_data, default='SCINet_HyParamTune_Realistic_records.csv'
-# 3. change args.tuned_hyperparmas for tuned hyperparams and scale
-#     e.g. tune {'levels': [2,3,4],'window_size': [32,64,128,256,512],}
-
-##################################################################################################################
-
-def get_args():
-
-
-    args = parser.parse_args()
-
-    if not args.long_term_forecast:
-        args.concat_len = args.window_size - args.horizon
-
-    return args
-
-def get_model(args):
-
-    Exp=Exp_kiglis
-    exp=Exp(args)
-
-    return exp
-
-def Hyper_param_Tune():
-
-    torch.manual_seed(4321)  # reproducible
-    torch.cuda.manual_seed_all(4321)
-    torch.backends.cudnn.benchmark = False
-    torch.backends.cudnn.deterministic = True  # Can change it to False --> default: False
-    torch.backends.cudnn.enabled = True
-
-    # initialize wandb
-    args.log_id = f"{dt.now().strftime('%d_%h_%H_%M')}-{args.model_name}-{args.dataset_name}"
-    run = wandb.init(name=args.log_id, config=args)
-
-    # save the files defined in wandb_record_files
-    if args.save_files:
-        run.log_code(include_fn=wandb_record_files)
-
-    # hyper-params to be swept & change name of run and log_id
-    args.levels = wandb.config.levels
-    args.window_size = wandb.config.window_size
-    run.name = run.name+f"-l{args.levels}-wl{args.window_size}-{uuid.uuid4()}"
-    args.log_id = args.log_id+f"-l{args.levels}-wl{args.window_size}-{uuid.uuid4()}"
-
-    # create and watch model
-    exp = get_model(args)
-
-    before_train = dt.now().timestamp()
-    print("===================Normal-Start=========================")
-    normalize_statistic = exp.train()
-    after_train = dt.now().timestamp()
-    print(f'Training took {(after_train - before_train) / 60} minutes')
-    print("===================Normal-End=========================")
-
-if __name__ == '__main__':
-
-    args = get_args()
-    args.dataset_name = get_dataset_name(args.data)
-
-    ### prepare config
-    # load the tunued hyper-params according to following form:
-    #  parameters={
-    #               'levels': [2,3,4],
-    #               'window_size': [32,64,128,256,512],
-    #               }
-    sweep_name = args.dataset_name + f': '
-    param_dict = {}
-    tune_count = 1
-    for Name, Scale in args.tuned_hyperparmas.items():
-        param_dict[Name] = {'values': Scale}
-        sweep_name = sweep_name + f'{Name},'
-        tune_count *= len(Scale)
-    print(f'sweep name will be: {sweep_name}')
-    print(f'how many times will be tuned: {tune_count}')
+# Import objective relevant libraries
+import os
+import sys
+from typing import Dict
+import os, sys
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+# Import organization
+import numpy as np
+import scipy.sparse as ssp
+import torch
 
+import matplotlib.pyplot as plt
+from lpda.adjacency import plot_coo_matrix, construct_sparse_adj
+
+from ogb.linkproppred import PygLinkPropPredDataset, Evaluator
+from heuristic.eval import (
+    evaluate_auc,
+    evaluate_hits,
+    evaluate_mrr,
+    get_metric_score,
+    get_prediction
+)
+
+from torch_geometric.graphgym.cmd_args import parse_args
+from torch_geometric.graphgym.config import cfg
+import numpy as np
+from sklearn.linear_model import LogisticRegression
+from sklearn.manifold import TSNE
+import random 
+from numba.typed import List
+from torch_geometric.utils import to_scipy_sparse_matrix    
+from Embedding.node2vec_tagplus import node2vec, set_cfg, data_loader
+from yacs.config import CfgNode as CN
+
+wandb.login()
+
+# 1: Define objective/training function
+def objective(config):
+    with wandb.init(config=config):
+        config = wandb.config
+        
+        cfg_file = "configs/pubmed/node2vec.yaml"
+        # # Load args file
+        with open(cfg_file, "r") as f:
+            args = CN.load_cfg(f)
+        
+
+        # Set Pytorch environment
+        torch.set_num_threads(args.num_threads)
+        
+        dataset, data_cited, splits = data_loader[args.data.name](args)
+        
+        # ust test edge_index as full_A
+        full_edge_index = splits['test'].edge_index
+        full_edge_weight = torch.ones(full_edge_index.size(1))
+        num_nodes = dataset._data.num_nodes
+        
+        m = construct_sparse_adj(full_edge_index)
+        plot_coo_matrix(m, f'test_edge_index.png')
+        
+        full_A = ssp.csr_matrix((full_edge_weight.view(-1), (full_edge_index[0], full_edge_index[1])), shape=(num_nodes, num_nodes)) 
+
+        # Access individual parameters
+        walk_length = config.walk_length
+        # `num_walks = args.model.node2vec.num_walks` is extracting the number of walks to be performed
+        # for each node in the Node2Vec algorithm from the argsuration settings. This parameter controls
+        # the number of random walks to be generated starting from each node in the graph during the node
+        # embedding generation process.
+        num_walks = config.num_walks
+        p = config.p
+        q = config.q
+        
+        embed_size = config.embed_size
+        ws = config.window_size
+        iter = config.iter
+    
+        # G = nx.from_scipy_sparse_matrix(full_A, create_using=nx.Graph())
+        adj = to_scipy_sparse_matrix(full_edge_index)
 
-    ### customized hyper-params tuning
-    sweep_configuration = {
-    'method': 'grid',
-    'name': sweep_name,
-    'metric': {'goal': 'minimize', 'name': 'test/ber'},
-    'parameters': param_dict
-    }
+        embed = node2vec(adj, embedding_dim=embed_size, walk_length=walk_length, walks_per_node=num_walks,
+                  workers=8, window_size=ws, num_neg_samples=1, p=p, q=q)
     
-    sweep_id = wandb.sweep(sweep=sweep_configuration, project=args.project, entity=args.entity)
-    wandb.agent(sweep_id, function=Hyper_param_Tune, count=tune_count)
\ No newline at end of file
+        # TODO different methods to generate node embeddings
+        # embedding method 
+        X_train_index, y_train = splits['train'].edge_label_index.T, splits['train'].edge_label
+        # dot product
+        X_train = embed[X_train_index]
+        X_train = np.multiply(X_train[:, 1], (X_train[:, 0]))
+        X_test_index, y_test = splits['test'].edge_label_index.T, splits['test'].edge_label
+        # dot product 
+        X_test = embed[X_test_index]
+        X_test = np.multiply(X_test[:, 1], (X_test[:, 0]))
+        
+        
+        clf = LogisticRegression(solver='lbfgs',max_iter=iter, multi_class='auto')
+        clf.fit(X_train, y_train)
+
+        score = clf.score(X_test, y_test)
+    return score
+
+def main():
+    wandb.init(project="my-first-sweep")
+    score = objective(wandb.config)
+    wandb.log({"score": score})
+
+# 2: Define the search space
+sweep_config = {
+    "method": "random",
+    "metric": {"goal": "maximize", "name": "score"},
+    "parameters": {
+        "walk_length": {"max": 30, "min": 5, 'distribution': 'int_uniform'},
+        "num_walks": {"values": [40, 60, 80]},
+        "embed_size": {"max": 128, "min": 32, 'distribution': 'int_uniform'},
+        "window_size": {"max": 10, "min": 2, 'distribution': 'int_uniform'},
+        "p": {"max": 1, "min": 0.1, 'distribution': 'uniform'},
+        "q": {"max": 1, "min": 0.1, 'distribution': 'uniform'},
+        "ws": {"values": [3, 7, 9, 11]},
+        "iter": {"values": [1, 3, 7]},
+    },
+}
+
+# 3: Start the sweep
+sweep_id = wandb.sweep(sweep=sweep_config, project="embedding-sweep")
+import pprint
+
+pprint.pprint(sweep_config)
+wandb.agent(sweep_id, function=main, count=20)
\ No newline at end of file
diff --git a/core/LMs/lm_trainer.py b/core/LMs/lm_trainer.py
index f253ec0..cf9b986 100644
--- a/core/LMs/lm_trainer.py
+++ b/core/LMs/lm_trainer.py
@@ -71,8 +71,8 @@ class LMTrainer():
         #     print("Initialize using previous ckpt...")
         #     self.model.load_state_dict(torch.load(prev_ckpt))
 
-        self.model.config.dropout = self.dropout
-        self.model.config.attention_dropout = self.att_dropout
+        self.model.args.dropout = self.dropout
+        self.model.args.attention_dropout = self.att_dropout
 
         trainable_params = sum(p.numel()
                                for p in self.model.parameters() if p.requires_grad)
diff --git a/core/LMs/model.py b/core/LMs/model.py
index 7935bee..3075672 100644
--- a/core/LMs/model.py
+++ b/core/LMs/model.py
@@ -8,17 +8,17 @@ from core.utils import init_random_state
 
 class BertClassifier(PreTrainedModel):
     def __init__(self, model, n_labels, dropout=0.0, seed=0, cla_bias=True, feat_shrink=''):
-        super().__init__(model.config)
+        super().__init__(model.args)
         self.bert_encoder = model
         self.dropout = nn.Dropout(dropout)
         self.feat_shrink = feat_shrink
-        hidden_dim = model.config.hidden_size
+        hidden_dim = model.args.hidden_size
         self.loss_func = nn.CrossEntropyLoss(
             label_smoothing=0.3, reduction='mean')
 
         if feat_shrink:
             self.feat_shrink_layer = nn.Linear(
-                model.config.hidden_size, int(feat_shrink), bias=cla_bias)
+                model.args.hidden_size, int(feat_shrink), bias=cla_bias)
             hidden_dim = int(feat_shrink)
         self.classifier = nn.Linear(hidden_dim, n_labels, bias=cla_bias)
         init_random_state(seed)
@@ -51,7 +51,7 @@ class BertClassifier(PreTrainedModel):
 
 class BertClaInfModel(PreTrainedModel):
     def __init__(self, model, emb, pred, feat_shrink=''):
-        super().__init__(model.config)
+        super().__init__(model.args)
         self.bert_classifier = model
         self.emb, self.pred = emb, pred
         self.feat_shrink = feat_shrink
diff --git a/core/config_gnn.py b/core/config_gnn.py
index be263fc..397b291 100644
--- a/core/config_gnn.py
+++ b/core/config_gnn.py
@@ -1,6 +1,6 @@
 import os
 import argparse
-from yacs.config import CfgNode as CN
+from yacs.args import CfgNode as CN
 
 
 def set_cfg(cfg):
@@ -88,18 +88,18 @@ def set_cfg(cfg):
     return cfg
 
 
-# Principle means that if an option is defined in a YACS config object,
-# then your program should set that configuration option using cfg.merge_from_list(opts) and not by defining,
+# Principle means that if an option is defined in a YACS args object,
+# then your program should set that argsuration option using cfg.merge_from_list(opts) and not by defining,
 # for example, --train-scales as a command line argument that is then used to set cfg.TRAIN.SCALES.
 
 
 def update_cfg(cfg, args_str=None):
     parser = argparse.ArgumentParser()
-    parser.add_argument('--config', default="",
-                        metavar="FILE", help="Path to config file")
+    parser.add_argument('--args', default="",
+                        metavar="FILE", help="Path to args file")
     # opts arg needs to match set_cfg
     parser.add_argument("opts", default=[], nargs=argparse.REMAINDER,
-                        help="Modify config options using the command-line")
+                        help="Modify args options using the command-line")
 
     if isinstance(args_str, str):
         # parse from a string
@@ -110,9 +110,9 @@ def update_cfg(cfg, args_str=None):
     # Clone the original cfg
     cfg = cfg.clone()
 
-    # Update from config file
-    if os.path.isfile(args.config):
-        cfg.merge_from_file(args.config)
+    # Update from args file
+    if os.path.isfile(args.args):
+        cfg.merge_from_file(args.args)
 
     # Update from command line
     cfg.merge_from_list(args.opts)
diff --git a/core/configs/arxiv_2023/node2vec.yaml b/core/configs/arxiv_2023/node2vec.yaml
index bee0fb5..0ae2632 100644
--- a/core/configs/arxiv_2023/node2vec.yaml
+++ b/core/configs/arxiv_2023/node2vec.yaml
@@ -1,4 +1,4 @@
-# node2vec_config.yaml
+# node2vec_args.yaml
 
 out_dir: results
 metric_best: f1
@@ -16,8 +16,8 @@ model:
   node2vec:
     walk_length: 10
     num_walks: 80
-    p: 0.25
-    q: 4
+    p: 0.5
+    q: 0.5
     workers: 1
     use_rejection_sampling: 0
     embed_size: 64
diff --git a/core/configs/cora/node2vec.yaml b/core/configs/cora/node2vec.yaml
index 58e8814..39c1721 100644
--- a/core/configs/cora/node2vec.yaml
+++ b/core/configs/cora/node2vec.yaml
@@ -1,4 +1,4 @@
-# node2vec_config.yaml
+# node2vec_args.yaml
 
 out_dir: results
 metric_best: f1
@@ -26,7 +26,7 @@ model:
     
 num_threads: 11
 data:
-  name: pubmed
+  name: cora
   undirected: True
   include_negatives: True
   val_pct: 0.15
diff --git a/core/configs/pubmed/node2vec.yaml b/core/configs/pubmed/node2vec.yaml
index 58e8814..f13335c 100644
--- a/core/configs/pubmed/node2vec.yaml
+++ b/core/configs/pubmed/node2vec.yaml
@@ -1,4 +1,4 @@
-# node2vec_config.yaml
+# node2vec_args.yaml
 
 out_dir: results
 metric_best: f1
diff --git a/core/graphgps/custom_config.py b/core/graphgps/custom_config.py
index fb47aec..3fb2115 100644
--- a/core/graphgps/custom_config.py
+++ b/core/graphgps/custom_config.py
@@ -1,9 +1,9 @@
-from torch_geometric.graphgym.register import register_config
+from torch_geometric.graphgym.register import register_config 
 from yacs.config import CfgNode as CN
 
 
 def set_cfg_wandb(cfg):
-    """Weights & Biases tracker configuration.
+    """Weights & Biases tracker argsuration.
     """
 
     # WandB group
diff --git a/core/graphgps/custom_train.py b/core/graphgps/custom_train.py
index cd73df7..aff72ed 100644
--- a/core/graphgps/custom_train.py
+++ b/core/graphgps/custom_train.py
@@ -3,7 +3,7 @@ from yacs.config import CfgNode as CN
 
 
 def set_cfg_train(cfg):
-    """Weights & Biases tracker configuration.
+    """Weights & Biases tracker argsuration.
     """
 
     # WandB group
diff --git a/core/heuristic/arxiv2023_heuristic.py b/core/heuristic/arxiv2023_heuristic.py
index f607ddd..379bd1c 100644
--- a/core/heuristic/arxiv2023_heuristic.py
+++ b/core/heuristic/arxiv2023_heuristic.py
@@ -32,12 +32,12 @@ from utils import get_git_repo_root_path, append_acc_to_excel, append_mrr_to_exc
 from heuristic.semantic_similarity import pairwise_prediction
 
 
-def get_raw_text_arxiv_2023(config):
-    undirected = config.data.undirected
-    include_negatives = config.data.include_negatives
-    val_pct = config.data.val_pct
-    test_pct = config.data.test_pct
-    split_labels = config.data.split_labels
+def get_raw_text_arxiv_2023(args):
+    undirected = args.data.undirected
+    include_negatives = args.data.include_negatives
+    val_pct = args.data.val_pct
+    test_pct = args.data.test_pct
+    split_labels = args.data.split_labels
     
     data = torch.load(FILE_PATH + 'dataset/arxiv_2023/graph.pt')
     
diff --git a/core/heuristic/cora_heuristic.py b/core/heuristic/cora_heuristic.py
index 464421b..21447cf 100644
--- a/core/heuristic/cora_heuristic.py
+++ b/core/heuristic/cora_heuristic.py
@@ -189,12 +189,12 @@ def parse_cora():
     return data_X, data_Y, data_citeid, np.unique(data_edges, axis=0).transpose()
 
 
-def get_cora_casestudy(config) -> InMemoryDataset:
-    undirected = config.data.undirected
-    include_negatives = config.data.include_negatives
-    val_pct = config.data.val_pct
-    test_pct = config.data.test_pct
-    split_labels = config.data.split_labels
+def get_cora_casestudy(args) -> InMemoryDataset:
+    undirected = args.data.undirected
+    include_negatives = args.data.include_negatives
+    val_pct = args.data.val_pct
+    test_pct = args.data.test_pct
+    split_labels = args.data.split_labels
     
     data_X, data_Y, data_citeid, data_edges = parse_cora()
 
diff --git a/core/heuristic/pubmed_heuristic.py b/core/heuristic/pubmed_heuristic.py
index 7ad7729..ccc4789 100644
--- a/core/heuristic/pubmed_heuristic.py
+++ b/core/heuristic/pubmed_heuristic.py
@@ -32,13 +32,13 @@ from heuristic.semantic_similarity import pairwise_prediction
 FILE_PATH = get_git_repo_root_path() + '/'
 
 
-def get_pubmed_casestudy(config):
+def get_pubmed_casestudy(args):
     corrected = False
-    undirected = config.data.undirected
-    include_negatives = config.data.include_negatives
-    val_pct = config.data.val_pct
-    test_pct = config.data.test_pct
-    split_labels = config.data.split_labels
+    undirected = args.data.undirected
+    include_negatives = args.data.include_negatives
+    val_pct = args.data.val_pct
+    test_pct = args.data.test_pct
+    split_labels = args.data.split_labels
     
     _, data_X, data_Y, data_pubid, data_edges = parse_pubmed()
     data_X = normalize(data_X, norm="l1")
diff --git a/core/loaders/lmlp_trainer.py b/core/loaders/lmlp_trainer.py
index 3da0699..9aa5941 100644
--- a/core/loaders/lmlp_trainer.py
+++ b/core/loaders/lmlp_trainer.py
@@ -63,8 +63,8 @@ class LMTrainer():
         #     print("Initialize using previous ckpt...")
         #     self.model.load_state_dict(torch.load(prev_ckpt))
 
-        self.model.config.dropout = self.dropout
-        self.model.config.attention_dropout = self.att_dropout
+        self.model.args.dropout = self.dropout
+        self.model.args.attention_dropout = self.att_dropout
 
         trainable_params = sum(p.numel()
                                for p in self.model.parameters() if p.requires_grad)
@@ -167,7 +167,7 @@ class LMTrainer():
 import os, sys
 sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
 from core.LMs.lm_trainer import LMTrainer
-from core.config import cfg, update_cfg
+from core.args import cfg, update_cfg
 import pandas as pd
 
 
diff --git a/core/scripts/heuristic.sh b/core/scripts/heuristic.sh
index 94c2087..f1d1f11 100644
--- a/core/scripts/heuristic.sh
+++ b/core/scripts/heuristic.sh
@@ -18,6 +18,6 @@ python arxiv_heuristic.py
 python ogbn_products_heuristic.py
 python pubmed_heuristic.py
 
-python core/Embedding/node2vec_tag.py --cfg core/configs/cora/node2vec.yaml 
-python core/Embedding/node2vec_tag.py --cfg core/configs/pubmed/node2vec.yaml 
-python core/Embedding/node2vec_tag.py --cfg core/configs/arxiv_2023/node2vec.yaml 
\ No newline at end of file
+python core/Embedding/node2vec_tag.py --cfg core/argss/cora/node2vec.yaml 
+python core/Embedding/node2vec_tag.py --cfg core/argss/pubmed/node2vec.yaml 
+python core/Embedding/node2vec_tag.py --cfg core/argss/arxiv_2023/node2vec.yaml 
\ No newline at end of file
diff --git a/core/trainEnsemble.py b/core/trainEnsemble.py
index 17321fd..99a085f 100644
--- a/core/trainEnsemble.py
+++ b/core/trainEnsemble.py
@@ -1,4 +1,4 @@
-from core.config import cfg, update_cfg
+from core.args import cfg, update_cfg
 from core.GNNs.ensemble_trainer import EnsembleTrainer
 import pandas as pd
 
diff --git a/core/trainGNN.py b/core/trainGNN.py
index cce8481..f0c6aab 100644
--- a/core/trainGNN.py
+++ b/core/trainGNN.py
@@ -1,7 +1,7 @@
 from core.GNNs.gnn_trainer import GNNTrainer
 from core.GNNs.dgl_gnn_trainer import DGLGNNTrainer
 import pandas as pd
-from core.config import cfg, update_cfg
+from core.args import cfg, update_cfg
 import time
 
 
diff --git a/core/trainLM.py b/core/trainLM.py
index ebc22c1..3adb57a 100644
--- a/core/trainLM.py
+++ b/core/trainLM.py
@@ -1,7 +1,7 @@
 import os, sys
 sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
 from core.LMs.lm_trainer import LMTrainer
-from core.config import cfg, update_cfg
+from core.args import cfg, update_cfg
 import pandas as pd
 
 
diff --git a/core/utils.py b/core/utils.py
index 7655911..5608ea4 100644
--- a/core/utils.py
+++ b/core/utils.py
@@ -121,60 +121,64 @@ def append_acc_to_excel(metrics_acc, root, name):
     
     csv_columns = ['Metric'] + list(k for k in metrics_acc) 
 
+    # load old csv
     try:
-        Data = pd.read_csv(root)[:1]
+        Data = pd.read_csv(root)[:-1]
     except:
         Data = pd.DataFrame(None, columns=csv_columns)
         Data.to_csv(root, index=False)
     
-    # transform dict to df    
-    id = uuid.uuid4()
+    # create new line 
+    uuid_val = uuid.uuid4()
     acc_lst = []
     
     for k, v in metrics_acc.items():
         acc_lst.append(process_value(v))
         
-    
-    v_lst = [f'{name}_{id}'] + acc_lst
+    # merge with old lines, 
+    v_lst = [f'{name}_{uuid_val}'] + acc_lst
     new_df = pd.DataFrame([v_lst], columns=csv_columns)
-    Data = pd.concat([Data, new_df])
-    highest_values = Data.apply(lambda column: max(column, default=None))
+    new_Data = pd.concat([Data, new_df])
+    
+    # best value
+    highest_values = new_Data.apply(lambda column: max(column, default=None))
 
+    # concat and save
     Best_list = ['Best'] + highest_values[1:].tolist()
     Best_df = pd.DataFrame([Best_list], columns=Data.columns)
-    Data = pd.concat([Data, Best_df])
-    Data.to_csv(root,index=False)
+    upt_Data = pd.concat([new_Data, Best_df])
+    upt_Data.to_csv(root,index=False)
 
-    return Data
+    return upt_Data
 
 
 def append_mrr_to_excel(metrics_mrr, root):
-    # if not exists save the first row
-    # transform dict to df    
+ 
+    uuid_val = uuid.uuid4()
     csv_columns, csv_numbers = [], []
     for i, (k, v) in enumerate(metrics_mrr.items()): 
         if i == 0:
             csv_columns = ['Metric'] + list(v.keys())
-        csv_numbers.append([k] + list(v.values()))
+        csv_numbers.append([f'{k}_{uuid_val}'] + list(v.values()))
     
     print(csv_numbers)
 
     try:
-        Data = pd.read_csv(root)[:1]
+        Data = pd.read_csv(root)[:-1]
     except:
         Data = pd.DataFrame(None, columns=csv_columns)
         Data.to_csv(root, index=False)
 
     
     new_df = pd.DataFrame(csv_numbers, columns = csv_columns)
-    Data = pd.concat([Data, new_df])
+    new_Data = pd.concat([Data, new_df])
     
-    highest_values = Data.apply(lambda column: max(column, default=None))
+    highest_values = new_Data.apply(lambda column: max(column, default=None))
     Best_list = ['Best'] + highest_values[1:].tolist()
     Best_df = pd.DataFrame([Best_list], columns=csv_columns)
-    Data = pd.concat([Data, Best_df])
+    upt_Data = pd.concat([new_Data, Best_df])
     
-    Data.to_csv(root, index=False)
+    upt_Data.to_csv(root, index=False)
 
     
-    return Data
\ No newline at end of file
+    return upt_Data
\ No newline at end of file
diff --git a/core/wandb/latest-run b/core/wandb/latest-run
index 7d407de..a654ea3 120000
--- a/core/wandb/latest-run
+++ b/core/wandb/latest-run
@@ -1 +1 @@
-run-20240118_213049-uz56h80h
\ No newline at end of file
+run-20240302_091048-mo5cwqj2
\ No newline at end of file
